{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af908bf5-0a34-4ace-93be-2f132c09a9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Processamento de Dados\n",
    "Este notebook pertence ao projeto: Case Téc­ni­co Data Sci­en­ce - iFo­od acessível em \\\n",
    "https://github.com/paulolaque/ifood-case.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ff5740-cf37-49e3-b05b-9c498f048a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Procedimentos iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac6c3d1-f53a-46a1-b181-b757dc3fdac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3862d9b2-600d-4cad-824d-ede52d10ffaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Este notebook executa em pyspark, favor instalar caso necessário\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    lit,\n",
    "    explode,\n",
    "    collect_set,\n",
    "    collect_list,\n",
    "    array,\n",
    "    array_contains,\n",
    "    sum as spark_sum,\n",
    "    isnan,\n",
    "    to_date,\n",
    "    datediff,\n",
    "    max as spark_max,\n",
    "    year,\n",
    "    month,\n",
    "    weekofyear,\n",
    "    struct\n",
    ")\n",
    "except ImportError:\n",
    "    print(\"PySpark não está instalado. Instale com: pip install pyspark\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e71a53-d128-43d5-97ce-e1016068140b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuração de diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f777ca3f-3bd1-4b25-8bda-12b5a729302a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detectar se está rodando no Databricks, pois o padrão de diretórios podem ser diferentes de uma máquina local\n",
    "running_in_databricks = os.path.exists(\"/databricks/driver\")\n",
    "\n",
    "# Ajusta o caminho raiz do projeto\n",
    "if running_in_databricks:\n",
    "    project_root = \"/dbfs\" + os.getcwd().split(\"/notebooks\")[0]  # raiz do workspace em DBFSpara Databricks\n",
    "else:\n",
    "    project_root = os.path.abspath(\"..\")  # pasta acima de notebooks/\n",
    "\n",
    "# Cria string de diretório\n",
    "raw_data_path = os.path.join(project_root, \"data\", \"raw\")\n",
    "tar_gz_path = os.path.join(raw_data_path, \"ds-technical-evaluation-data.tar.gz\")\n",
    "\n",
    "# Cria a pasta se não existir\n",
    "os.makedirs(raw_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268842c8-f6a7-488f-ab54-065832a5705b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuração de sessão Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda8cfca-75fc-440c-85d1-ef97c119bf13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria SparkSession se necessário\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load JSON Files\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c263ff77-079e-42ac-a988-e03f32804be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carregamento e Leitura dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b3efc8-1f53-44ad-b5d2-7f73d0332318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Download e extração de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3193f8-7fa3-49b6-acf2-9ae20b21d69a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existe.\n",
      "Extraindo arquivos...\n",
      "Extração completa.\n",
      "Arquivos extraídos:\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._transactions.json\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._offers.json\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/profile.json\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/offers.json\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._profile.json\n",
      "/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/transactions.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL de download\n",
    "url = \"https://data-architect-test-source.s3.sa-east-1.amazonaws.com/ds-technical-evaluation-data.tar.gz\"\n",
    "\n",
    "# Baixa o arquivo se não existir\n",
    "if not os.path.exists(tar_gz_path):\n",
    "    print(\"Baixando arquivo...\")\n",
    "    urllib.request.urlretrieve(url, tar_gz_path)\n",
    "    print(\"Download completo.\")\n",
    "else:\n",
    "    print(\"Arquivo já existe.\")\n",
    "\n",
    "# Extrai os arquivos\n",
    "print(\"Extraindo arquivos...\")\n",
    "with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=raw_data_path)\n",
    "print(\"Extração completa.\")\n",
    "\n",
    "# Lista arquivos\n",
    "extracted_path = os.path.join(raw_data_path, \"ds-technical-evaluation-data\")\n",
    "print(\"Arquivos extraídos:\")\n",
    "for root, dirs, files in os.walk(extracted_path):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbbedf2-504c-4a1d-9d25-a61c3365bca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Conversão para DataFrames do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8dc4f64-f115-4cf7-84e1-88f7c6dbbbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "._transactions.json\n",
      "._offers.json\n",
      "profile.json\n",
      "offers.json\n",
      "._profile.json\n",
      "transactions.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(extracted_path):\n",
    "    print(file)\n",
    "\n",
    "\n",
    "# Função para extrair em tabelas\n",
    "def read_extracted_df(table_name: str):\n",
    "    df = None  # Inicializa a variável df\n",
    "    \n",
    "    for file in os.listdir(extracted_path):\n",
    "        # Verifica se o arquivo é o '.json'\n",
    "        if table_name in file.lower() and file.endswith(\".json\"):\n",
    "            # Detecta se precisa usar prefixo \"file:/\"\n",
    "            if running_in_databricks:\n",
    "                spark_path = \"file:\" + os.path.join(extracted_path, file)\n",
    "            else:\n",
    "                spark_path = os.path.join(extracted_path, file)\n",
    "\n",
    "            print(f\"Lendo {spark_path}\")\n",
    "\n",
    "            try:\n",
    "                # Lê o arquivo JSON\n",
    "                df = spark.read.json(spark_path)\n",
    "                df = df.withColumn(\"source_file\", lit(file))  # Adiciona a coluna do nome do arquivo\n",
    "                break  # Para ao encontrar o arquivo \".json\"\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler {file}: {e}\")\n",
    "                \n",
    "    return df  # Retorna o DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730d289b-ea2c-4fb3-b52d-4871360369c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo /workspaces/ifood-case/data/raw/ds-technical-evaluation-data/profile.json\n",
      "Lendo /workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._transactions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 17:08:26 WARN DataSource: All paths were ignored:\n",
      "  file:/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._transactions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao ler ._transactions.json: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for JSON. It must be specified manually.\n",
      "Lendo /workspaces/ifood-case/data/raw/ds-technical-evaluation-data/transactions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 17:08:28 WARN DataSource: All paths were ignored:                      \n",
      "  file:/workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._offers.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo /workspaces/ifood-case/data/raw/ds-technical-evaluation-data/._offers.json\n",
      "Erro ao ler ._offers.json: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for JSON. It must be specified manually.\n",
      "Lendo /workspaces/ifood-case/data/raw/ds-technical-evaluation-data/offers.json\n"
     ]
    }
   ],
   "source": [
    "df_profile = read_extracted_df('profile')\n",
    "df_transactions=read_extracted_df('transactions')\n",
    "df_offers=read_extracted_df('offers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab38441-7ab6-44e6-85a8-e49a5c96cd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformação de Tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41eb4f7c-a2a5-4886-8b43-221e35df5dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Desaninhamento de JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183f51b4-73cc-46b9-930e-0f8d7a410a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: 306534\n",
      "Depois de desaninhar Json da coluna value: 306534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Antes:', df_transactions.count())\n",
    "\n",
    "# Acesse diretamente os campos da coluna STRUCT 'value'\n",
    "df_transactions = df_transactions.select(\"*\", \n",
    "    col(\"value.amount\").alias(\"amount\"),\n",
    "    col(\"value.offer_id\").alias(\"offer_id\"),\n",
    "    col(\"value.offer id\").alias(\"offer id\"),\n",
    "    col(\"value.reward\").alias(\"reward\")\n",
    ")\n",
    "\n",
    "print(f'Depois de desaninhar Json da coluna value: {df_transactions.count()}')\n",
    "\n",
    "# Retirar coluna desaninhada\n",
    "df_transactions=df_transactions.drop(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c672e52-e4b8-4fd1-a620-ea74f74f8e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podemos ver que desaninhar não alterou o número de linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03509836-49a0-421e-afe6-57adcdf3d7e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Conversão de arrays em colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6a15e2-d042-459e-b136-f751fc7c4a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Explode os valores da lista para identificar os valores únicos\n",
    "unique_channels = df_offers.select(explode(col(\"channels\")).alias(\"channel\")) \\\n",
    "                                  .distinct() \\\n",
    "                                  .rdd.flatMap(lambda row: row) \\\n",
    "                                  .collect()\n",
    "\n",
    "#  Para cada valor possível, cria uma nova coluna binária (1 se presente, 0 se não)\n",
    "for ch in unique_channels:\n",
    "    df_offers = df_offers.withColumn(\n",
    "        f\"channel_{ch}\",\n",
    "        when(array_contains(col(\"channels\"), ch), lit(1)).otherwise(lit(0))\n",
    "    )\n",
    "df_offers=df_offers.drop(\"channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085b6c3d-1042-4653-b9bf-17b5714ac578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulação e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7e7c44-94c7-4735-9f8d-306c8a76f657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remover linhas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192d8c0c-1629-4343-92ce-6d4767e48165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " df_transactions:\n",
      " - Total de linhas: 306534\n",
      " - Linhas distintas: 306137\n",
      " - Linhas duplicadas: 397\n",
      "\n",
      " df_offers:\n",
      " - Total de linhas: 10\n",
      " - Linhas distintas: 10\n",
      " - Linhas duplicadas: 0\n",
      "\n",
      " df_profile:\n",
      " - Total de linhas: 17000\n",
      " - Linhas distintas: 17000\n",
      " - Linhas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "def verificar_duplicatas(df, nome_df=\"DataFrame\"):\n",
    "    total = df.count()\n",
    "    distintos = df.distinct().count()\n",
    "    duplicadas = total - distintos\n",
    "    print(f\"\\n {nome_df}:\")\n",
    "    print(f\" - Total de linhas: {total}\")\n",
    "    print(f\" - Linhas distintas: {distintos}\")\n",
    "    print(f\" - Linhas duplicadas: {duplicadas}\")\n",
    "\n",
    "verificar_duplicatas(df_transactions, \"df_transactions\")\n",
    "verificar_duplicatas(df_offers, \"df_offers\")\n",
    "verificar_duplicatas(df_profile, \"df_profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cdc34a-6b2f-455e-a672-30fca9cdbb07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Como temos uma série histórica de eventos sem id por evento (a tabela transactions não tem id) e temos apenas o tempo (time_since_test_start), o tipo de evento e o id da conta como referência, não conseguimos identificar se um evento de fato esta duplicado na tabela. Porem, como verificamos que apenas 0,13% da base esta duplicada, acredito que não será significante na nossa base. Por isso vamos excluir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2ba672-d666-45da-87be-9fbb7c2b8f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " df_transactions:\n",
      " - Total de linhas: 306137\n",
      " - Linhas distintas: 306137\n",
      " - Linhas duplicadas: 0\n",
      "\n",
      " df_offers:\n",
      " - Total de linhas: 10\n",
      " - Linhas distintas: 10\n",
      " - Linhas duplicadas: 0\n",
      "\n",
      " df_profile:\n",
      " - Total de linhas: 17000\n",
      " - Linhas distintas: 17000\n",
      " - Linhas duplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "df_transactions = df_transactions.dropDuplicates()\n",
    "df_offers = df_offers.dropDuplicates()\n",
    "df_profile = df_profile.dropDuplicates()\n",
    "\n",
    "verificar_duplicatas(df_transactions, \"df_transactions\")\n",
    "verificar_duplicatas(df_offers, \"df_offers\")\n",
    "verificar_duplicatas(df_profile, \"df_profile\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237c4310-9c18-4dc9-9ac1-888fb59a8f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identificar colunas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d27b3a-14a6-4a85-a3e4-ca624e2d3276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, account_id: string, event: string, time_since_test_start: string, source_file: string, amount: string, offer_id: string, offer id: string, reward: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_transactions.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2249ff-aa56-417f-a306-c352584b20fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 279:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas com 'offer id' diferente de 'offer_id': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Como offer id e offer_id aparentemente são colunas que se complementam vamos testar se isso é verdade: quando ambas são não nulas se não temos divergência de valores\n",
    "\n",
    "# Filtra linhas em que ambas as colunas são não nulas e diferentes\n",
    "df_diff = df_transactions.filter(\n",
    "    col(\"offer id\").isNotNull() & \n",
    "    col(\"offer_id\").isNotNull() & \n",
    "    (col(\"offer id\") != col(\"offer_id\"))\n",
    ")\n",
    "\n",
    "# Conta essas linhas\n",
    "diff_count = df_diff.count()\n",
    "print(f\"Número de linhas com 'offer id' diferente de 'offer_id': {diff_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a1344f-baa9-44b3-a005-babba521dfd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, credit_card_limit: string, gender: string, id: string, registered_on: string, source_file: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_profile.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc73bb18-c474-403c-b227-68107f299fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, discount_value: string, duration: string, id: string, min_value: string, offer_type: string, source_file: string, channel_mobile: string, channel_email: string, channel_social: string, channel_web: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_offers.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd325aa7-5242-47cd-81f0-30cff76974e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Apenas olhandos o mínimo e máximo de cada coluna podemos ver que não temos possíveis colunas duplicadas.\n",
    " Se fossem iguais poderiamos testar se realmente são iguais,mas não precisamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf22911c-1a55-4ce7-9d9f-35983795789f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tratamento de valores nulos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52a48e8-cdd0-4622-aaa8-04d4f5045cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verificação de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53806671-175e-4dd5-b91a-e25abacf9fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Verificando nulos em: df_profile\n",
      "+---+-----------------+------+---+-------------+-----------+\n",
      "|age|credit_card_limit|gender| id|registered_on|source_file|\n",
      "+---+-----------------+------+---+-------------+-----------+\n",
      "|  0|             2175|  2175|  0|            0|          0|\n",
      "+---+-----------------+------+---+-------------+-----------+\n",
      "\n",
      "\n",
      ">>> Verificando nulos em: df_transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------------------+-----------+------+--------+--------+------+\n",
      "|account_id|event|time_since_test_start|source_file|amount|offer_id|offer id|reward|\n",
      "+----------+-----+---------------------+-----------+------+--------+--------+------+\n",
      "|         0|    0|                    0|          0|167184|  272955|  172135|272955|\n",
      "+----------+-----+---------------------+-----------+------+--------+--------+------+\n",
      "\n",
      "\n",
      ">>> Verificando nulos em: df_offers\n",
      "+--------------+--------+---+---------+----------+-----------+--------------+-------------+--------------+-----------+\n",
      "|discount_value|duration| id|min_value|offer_type|source_file|channel_mobile|channel_email|channel_social|channel_web|\n",
      "+--------------+--------+---+---------+----------+-----------+--------------+-------------+--------------+-----------+\n",
      "|             0|       0|  0|        0|         0|          0|             0|            0|             0|          0|\n",
      "+--------------+--------+---+---------+----------+-----------+--------------+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def check_nulls(df, df_name=\"DataFrame\"):\n",
    "    print(f\"\\n>>> Verificando nulos em: {df_name}\")\n",
    "    nulls_df = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "    ])\n",
    "    nulls_df.show()\n",
    "\n",
    "dfs = {\n",
    "    \"df_profile\": df_profile,\n",
    "    \"df_transactions\": df_transactions,\n",
    "    \"df_offers\": df_offers\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    check_nulls(df, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bcca0c-4040-4f86-a43c-e5d6d23f541e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Imputação de valores nulos em numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af026a9b-e906-438a-8bf9-90a45a5aea73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos das colunas:\n",
      "df_profile:\n",
      "root\n",
      " |-- credit_card_limit: double (nullable = true)\n",
      "\n",
      "df_transactions:\n",
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- reward: double (nullable = true)\n",
      "\n",
      "\n",
      "Número de registros com credit_card_limit = 0: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 303:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros com amount = 0: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 306:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros com reward = 0: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verificando a hipótese de que os valores nulos são na verdade zero nas colunas numéricas\n",
    "print(\"Tipos das colunas:\")\n",
    "print(\"df_profile:\")\n",
    "df_profile.select(\"credit_card_limit\").printSchema()\n",
    "\n",
    "print(\"df_transactions:\")\n",
    "df_transactions.select(\"amount\", \"reward\").printSchema()\n",
    "\n",
    "# Contando valores zero\n",
    "zero_credit_card_limit = df_profile.filter(col(\"credit_card_limit\") == 0).count()\n",
    "print(f\"\\nNúmero de registros com credit_card_limit = 0: {zero_credit_card_limit}\")\n",
    "\n",
    "zero_amount = df_transactions.filter(col(\"amount\") == 0).count()\n",
    "print(f\"Número de registros com amount = 0: {zero_amount}\")\n",
    "\n",
    "zero_reward = df_transactions.filter(col(\"reward\") == 0).count()\n",
    "print(f\"Número de registros com reward = 0: {zero_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a353d6-8aa9-4779-9065-09d7b4fd0c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Como não temos valor zero para as três colunas numéricas com dados faltantes (reward,amount e credit_card_limit), faz sentido supor que quando são nulas são zero, logo vamos inputar como zero esses dados. Para não perder informação vamos criar colunas informando quando elas foram nulas.\n",
    "\n",
    "\n",
    "# Para a tabela df_profile\n",
    "df_profile = df_profile.withColumn(\n",
    "    \"credit_card_limit_is_null\", \n",
    "    when(col(\"credit_card_limit\").isNull(), 1).otherwise(0)\n",
    ").fillna({\"credit_card_limit\": 0})  # Preenche os nulos de credit_card_limit com 0\n",
    "\n",
    "# Para a tabela df_transactions\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"amount_is_null\", \n",
    "    when(col(\"amount\").isNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"reward_is_null\", \n",
    "    when(col(\"reward\").isNull(), 1).otherwise(0)\n",
    ").fillna({\"amount\": 0, \"reward\": 0})  # Preenche os nulos de amount e reward com 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef200a1f-3848-4592-9f5d-6993afb22871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Imputação de valores nulos com coluna auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d9f8bb-560c-41df-96a0-22cef75d2b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Como vimos que 'offer id' e 'offer_id' se complementam e não contem divergência de dados vamos unificá-las\n",
    "# Atualiza 'offer_id' com valor de 'offer id' se for null\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"offer_id\",\n",
    "    when(col(\"offer_id\").isNull(), col(\"offer id\")).otherwise(col(\"offer_id\"))\n",
    ")\n",
    "\n",
    "# Remove a coluna com dados duplicados\n",
    "df_transactions = df_transactions.drop(\"offer id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70c7178-24e4-4230-930d-eaca9c2293da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Imputação de valores nulos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6e9093-d0a5-4735-8ede-094938d17f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_profile=df_profile.fillna({\n",
    "    \"gender\": \"nao_preenchido\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1657410-a050-4d1b-841a-c84f5b721caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remoção de colunas irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb73ced-4d36-412b-b93f-f335d5d08dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#vamos também remover as colunas source que não adicionam valor a nossa análise por ter sempre o mesmo valor e dizer apenas o nome da tabela. Isso vai melhorar a organização da tabela para não ter colunas com nomes duplicados\n",
    "df_transactions=df_transactions.drop(\"source_file\")\n",
    "df_offers=df_offers.drop(\"source_file\")\n",
    "df_profile=df_profile.drop(\"source_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031f33c2-bee4-4a97-9aa9-c2d13b4b76d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remoção de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9825e9ec-32fa-45ba-959b-4d2a9f20b36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removendo outliers em: df_transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 linhas removidas por outliers em df_transactions\n",
      "Total final de linhas: 306137\n",
      "\n",
      "Removendo outliers em: df_offers\n",
      "0 linhas removidas por outliers em df_offers\n",
      "Total final de linhas: 10\n",
      "\n",
      "Removendo outliers em: df_profile\n",
      "0 linhas removidas por outliers em df_profile\n",
      "Total final de linhas: 17000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def remover_outliers(df, nome_df=\"df\"):\n",
    "    print(f\"\\nRemovendo outliers em: {nome_df}\")\n",
    "    numeric_cols = [f.name for f in df.schema.fields if str(f.dataType) in ('IntegerType', 'DoubleType', 'LongType', 'FloatType')]\n",
    "    original_count = df.count()\n",
    "    df_filtrado = df\n",
    "\n",
    "    for col_name in numeric_cols:\n",
    "        quantiles = df_filtrado.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "        if len(quantiles) < 2:\n",
    "            continue\n",
    "\n",
    "        q1, q3 = quantiles\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "\n",
    "        df_filtrado = df_filtrado.filter((col(col_name) >= lower) & (col(col_name) <= upper))\n",
    "\n",
    "    final_count = df_filtrado.count()\n",
    "    total_removed = original_count - final_count\n",
    "\n",
    "    print(f\"{total_removed} linhas removidas por outliers em {nome_df}\")\n",
    "    print(f\"Total final de linhas: {final_count}\")\n",
    "    return df_filtrado\n",
    "\n",
    "# Aplicando a função nos três datasets\n",
    "df_transactions = remover_outliers(df_transactions, \"df_transactions\")\n",
    "df_offers = remover_outliers(df_offers, \"df_offers\")\n",
    "df_profile = remover_outliers(df_profile, \"df_profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e52559-c1cc-45b1-99b6-f66dea56d72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preparação do Dataset Unificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1618ff00-4e5d-4b7e-b9f6-63dcd9fce54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Junção de múltiplas tabelas (joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84affae2-2f88-40ec-834a-4fc243aae92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas Antes: 306137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 351:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas depois: 306137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Faz o LEFT JOIN entre transações e perfis e remover colunas de ids duplicados.\n",
    "print('Total de linhas Antes:',df_transactions.count())\n",
    "df_joined = df_transactions.join(df_profile, df_transactions.account_id == df_profile.id, how=\"left\")\n",
    "df_joined=df_joined.drop(\"id\")\n",
    "df_joined = df_joined.join(df_offers, df_joined.offer_id == df_offers.id, how=\"left\")\n",
    "df_joined=df_joined.drop(\"id\")\n",
    "print('Total de linhas depois:',df_joined.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b6fc67-a39a-4fa3-8594-c2ca12e4afaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checagem de integridade após merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22cc2ed3-3d59-44fa-801e-5d32e98c27fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores nulos no df_joined:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[account_id: bigint, event: bigint, time_since_test_start: bigint, amount: bigint, offer_id: bigint, reward: bigint, amount_is_null: bigint, reward_is_null: bigint, age: bigint, credit_card_limit: bigint, gender: bigint, registered_on: bigint, credit_card_limit_is_null: bigint, discount_value: bigint, duration: bigint, min_value: bigint, offer_type: bigint, channel_mobile: bigint, channel_email: bigint, channel_social: bigint, channel_web: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def contar_nulos(df, nome_df=\"DataFrame\"):\n",
    "    print(f\"\\nValores nulos no {nome_df}:\\n\")\n",
    "    display(df.select([\n",
    "        spark_sum(\n",
    "            when(col(c).isNull(), 1).otherwise(0)\n",
    "        ).alias(c) for c in df.columns\n",
    "    ]))\n",
    "\n",
    "# Exemplo para df_joined\n",
    "contar_nulos(df_joined, \"df_joined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1569f218-11c8-498b-a38d-e66d511c5f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ajuste de Nulos Pós Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a279b29a-5c02-4cee-bab9-d384096a5943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Como na tabela transactions haviam offer_id nulos vamos prencher esses valores com zero, ou seja não houve oferta e vamos criar uma coluna para identificar quando não houve oferta.\n",
    "\n",
    "\n",
    "#  Criar coluna binária indicando se offer_id é nulo\n",
    "df_joined = df_joined.withColumn(\"is_offer_id_null\", when(col(\"offer_id\").isNull(), 1).otherwise(0))\n",
    "\n",
    "#  Lista de colunas para preencher nulos com zero\n",
    "colunas_para_preencher = [\n",
    "    \"offer_id\", \"discount_value\", \"duration\", \"min_value\",\n",
    "    \"channel_mobile\", \"channel_email\", \"channel_social\", \"channel_web\"\n",
    "]\n",
    "\n",
    "#  Substituir nulos por 0 nessas colunas\n",
    "for coluna in colunas_para_preencher:\n",
    "    df_joined = df_joined.withColumn(coluna, when(col(coluna).isNull(), 0).otherwise(col(coluna)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a297f3f-e0ae-4749-a91b-52efc0dd5250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Engenharia de Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d8d990-c533-40c1-9918-f287bd7c1e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformar Data em tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cd9c4f-9f4a-41d4-a897-27e41a8f1bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vamos criar a coluna de data para tempo para o modelo conseguir interpretar \n",
    "\n",
    "#  Converter 'registered_on' para data no formato yyyyMMdd\n",
    "df_joined = df_joined.withColumn(\"registered_on_date\", to_date(col(\"registered_on\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "\n",
    "#  Obter a data mais recente da coluna 'registered_on_date'\n",
    "max_data = df_joined.agg(spark_max(\"registered_on_date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# Criar nova coluna com diferença em dias até a data mais recente\n",
    "df_joined = df_joined.withColumn(\"dias_desde_registro\", datediff(lit(max_data), col(\"registered_on_date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "959920eb-d160-42f3-8350-5401cfadc6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extração de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e25fdb-e274-4235-be70-93b97c70344f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Criação de variáveis temporais derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23388fe-fbd9-423e-b316-cb48aef8c432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Criar colunas de ano, mês e semana\n",
    "df_joined = df_joined \\\n",
    "    .withColumn(\"ano_registro\", year(col(\"registered_on_date\"))) \\\n",
    "    .withColumn(\"mes_registro\", month(col(\"registered_on_date\"))) \\\n",
    "    .withColumn(\"semana_registro\", weekofyear(col(\"registered_on_date\")))\n",
    "\n",
    "# Remover coluna antiga\n",
    "df_joined = df_joined.drop(\"registered_on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe55281-fb34-4335-b0a0-3587705b01fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reformatação da Tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efddef4b-8dff-40d0-bae6-59f76c7b054a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Filtrar ofertas recebidas\n",
    "offers_received = df_joined.filter(col(\"event\") == \"offer received\")\n",
    "\n",
    "#  Adicionar janela de duração da oferta\n",
    "offers_received = offers_received.withColumn(\"offer_window_end\", col(\"time_since_test_start\") + col(\"duration\"))\n",
    "\n",
    "#  Transações\n",
    "transactions = df_joined.filter(col(\"event\") == \"transaction\") \\\n",
    "    .select(\"account_id\", \"time_since_test_start\") \\\n",
    "    .withColumnRenamed(\"time_since_test_start\", \"transaction_time\")\n",
    "\n",
    "#  Verificar se houve transação no intervalo da oferta\n",
    "joined = offers_received.join(\n",
    "    transactions,\n",
    "    on=\"account_id\",\n",
    "    how=\"left\"\n",
    ").filter(\n",
    "    (col(\"transaction_time\") >= col(\"time_since_test_start\")) &\n",
    "    (col(\"transaction_time\") <= col(\"offer_window_end\"))\n",
    ")\n",
    "\n",
    "#  Marcar conversões\n",
    "converted = joined.groupBy(\"account_id\", \"offer_id\", \"time_since_test_start\") \\\n",
    "    .agg(spark_max(lit(1)).alias(\"converted\"))\n",
    "\n",
    "#  Preencher 0 para quem não converteu\n",
    "offers = offers_received.join(\n",
    "    converted,\n",
    "    on=[\"account_id\", \"offer_id\", \"time_since_test_start\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\"converted\", when(col(\"converted\").isNull(), 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b9b1a5-0c3d-448e-b6cc-3fc4fe9aa0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b438e1-7c2f-413e-9626-be536086e116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salvar em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddaafea-bb60-4844-b44a-37b5c49e8471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando dados em: /workspaces/ifood-case/data/processed/unificado_transformed.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando dados em: /workspaces/ifood-case/data/processed/unificado.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 407:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    # Caminho para salvar os dados processados\n",
    "    processed_data_path = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "    # Cria a pasta se não existir\n",
    "    os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "    # Caminho completo do arquivo Parquet\n",
    "    parquet_output_path_transformed = os.path.join(processed_data_path, \"unificado_transformed.parquet\")\n",
    "    parquet_output_path = os.path.join(processed_data_path, \"unificado.parquet\")\n",
    "\n",
    "    # Salva como Parquet\n",
    "    print(f\"Salvando dados em: {parquet_output_path_transformed}\")\n",
    "    offers.write.mode(\"overwrite\").parquet(parquet_output_path_transformed)\n",
    "    print(f\"Salvando dados em: {parquet_output_path}\")\n",
    "    df_joined.write.mode(\"overwrite\").parquet(parquet_output_path)\n",
    "    print(\"Dados salvos com sucesso.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_data_processing.ipynb",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
